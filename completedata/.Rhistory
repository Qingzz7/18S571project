myFirstFunc(2, 3)
newFunc <- function(x = 5, y = 10){
return(x * y)
}
newFunc()
# List material adapted from http://faculty.nps.edu/sebuttre/home/R/lists.html
# R uses lists as the data type for many useful things
mylist <- list (a = 1:5, b = "Hi There", c = function(x) x * sin(x))
# Obtain the first item from the list
mylist[1]
mylist[3]
# Why does this fail?
mylist[1] * 5
# If we want to get the items out in their original form, we use [[]]
mylist[[1]]
mylist[[1]] * 5
# List material adapted from http://faculty.nps.edu/sebuttre/home/R/lists.html
# R uses lists as the data type for many useful things
mylist <- list (a = 1:5, b = "Hi There", c = function(x) x * sin(x))
# Or like this
mylist['a']
# Can also refer by name
mylist$a
# What's the second element of the item named "a"?
mylist$a[2]
# We can use negative indices too!
# Give me everything from "a" except the second element
mylist$a[-2]
# Let's add a new item to our list, in a slot we'll call "d"
mylist$d <- "New item"
mylist[4]
n = c(2, 3, 5)
s = c("aa", "bb", "cc")
b = c(TRUE, FALSE, TRUE)
df = data.frame(n, s, b)
# You can easily determine the number of rows and columns in a data.frame
nrow(df)
ncol(df)
df
# Best practice is to use the apply functions to perform operations on
# every element
?apply
?lappy
?lappy
?sapply
sapply()
# R provides some built in data.frames for practice
df <- mtcars
sapply(X = df[,'mpg'], FUN = sum)
View(df)
mpg = read.table('https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data', header = FALSE)
names(mpg) = c('mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'modelYear', 'origin', 'carName')
catv=names(mpg[,sapply(mpg,is.factor)])
catv
# [1] "horsepower" "carName"
# however, "horsepower" is numeric
catv=catv[2]
df[,'horsepower'] = as.numeric(df[,'horsepower'])
numv=names(mpg[,sapply(mpg,is.numeric)])
numv
numv=names(mpg[,sapply(mpg,is.numeric)])
numv
numv=names(mpg[,sapply(mpg,is.numeric)])
numv
numv=names(mpg[,sapply(mpg,is.numeric)])
numv
binv=names(mpg[,sapply(mpg,is.logical)])
binv
#character(0)
#character(0)
stopifnot(ncol(catv) + ncol(numv) +ncol(binv) == ncol(mpg)) # check if it includes all types data
# however, "horsepower" is numeric
catv=catv[2]
catv
catv=names(mpg[,sapply(mpg,is.factor)])
catv
# however, "horsepower" is numeric
catv=catv[2]
mpg[,'horsepower'] = as.numeric(mpg[,'horsepower'])
numv = c(numv, 'horsepower')
summary(mpg)
mpg_num = mpg[,numv]#get plots for numeric variables
boxplot(mpg_num)
pairs(~., data = mpg_num)
#For catgorical variables, we use frequency table.
levels(mpg$carName)
# Too many levels in these two variables. Making contingency table is not a good idea.
mpg_ct=table(mpg$carName)
mpg_ct
mpg_num = mpg[,numv]#get plots for numeric variables
mpg_cor=cor(mpg_num)
mpg_cor
library(corrplot)
corrplot.mixed(mpg_cor)
install.packages("corrplot")
library(corrplot)
corrplot.mixed(mpg_cor)
colnames(mpg)[colSums(mpg=="?")>0]
colnames(mpg)[colSums(is.na(mpg))>0]
summary(mpg)
# 1.Load in the auto mpg data set: https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data
mpg = read.table('https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data', header = FALSE)
names(mpg) = c('mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'modelYear', 'origin', 'carName')
summary(mpg)
colnames(mpg)[colSums(is.na(mpg))>0]
df <- read.table('https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data')
names(df) <- c('mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'modelYear', 'origin', 'carName')
# 2. Identify all of the categorical variables, all of the numeric variables
numVars <- names(which(sapply(df, is.numeric)))
catVars <- names(which(sapply(df, is.factor)))
catVars <- catVars[2]
df[,'horsepower'] <- as.numeric(df[,'horsepower'])
numVars <- c(numVars, 'horsepower')
logicalVars <- names(which(sapply(df, is.logical)))
targetVar <- 'mpg'
varsToRemove <- NA
xVars <- c(catVars, numVars, logicalVars)
xVars <-xVars[!xVars %in% c(targetVar, varsToRemove)]
# Descriptive statistics for the data set
summary(df)
missingDataCols <- colnames(df)[colSums(is.na(df)) > 0]
missingDataCols
View(mpg)
colnames(mpg)[colSums(mpg=="?")>0]
library('caret')
set.seed(2575) # to make result reproducible
itrain = createDataPartition(y = mpg[,1], p = .8,list = FALSE)
mpg_train=mpg[itrain,]
mpg_test=mpg[-itrain,]
stopifnot(nrow(mpg_train) + nrow(mpg_test) == nrow(mpg))# check if missed any row
numv
mpg_trainnum=mpg_train[,numv]
mpg_lm=lm(mpg_trainnum$mpg ~ ., data =mpg_trainnum) #build linear model based on training data
summary(mpg_lm)
library('caret')
trainPct = .8
inTrain <- createDataPartition(y = df[,targetVar], p = trainPct, list = FALSE)
train <- df[inTrain,]
test <- df[-inTrain,]
stopifnot(nrow(train) + nrow(test) == nrow(df))
createModelFormula <- function(targetVar, xVars, includeIntercept = TRUE){
if(includeIntercept){
modelForm <- as.formula(paste(targetVar, "~", paste(xVars, collapse = '+ ')))
} else {
modelForm <- as.formula(paste(targetVar, "~", paste(xVars, collapse = '+ '), -1))
}
return(modelForm)
}
# Remove mpg from numVars
numVars <- numVars[-1]
modelForm <- createModelFormula(targetVar = targetVar, xVars = numVars)
model1 <- lm(modelForm, data = train)
summary(model1)
mpg_testnum=mpg_test[,numv] #numerical varaible in test data set
y=mpg_testnum[,1] #actual target value in test data set
yhat=predict(mpg_lm,newdata = mpg_testnum)[,1] #predicted target value in test data set
rsq=sum((yhat - mean(y))**2)/sum((y - mean(y))**2) #calculate the e square
rsq
mpg[,'horsepower'] = as.numeric(mpg[,'horsepower'])
numv = c(numv, 'horsepower')
class(mpg$horsepower)
summary(mpg)
colnames(mpg)[colSums(mpg=="?")>0]
colnames(mpg)[colSums(is.na(mpg))>0]
colnames(mpg)[colSums(mpg=='?')>0]
mpg_trainnum=mpg_train[,numv]
mpg_lm=lm(mpg_trainnum$mpg ~ ., data =mpg_trainnum) #build linear model based on training data
summary(mpg_lm)
library('caret')
set.seed(2575) # to make result reproducible
itrain = createDataPartition(y = mpg[,1], p = .8,list = FALSE)
mpg_train=mpg[itrain,]
mpg_test=mpg[-itrain,]
stopifnot(nrow(mpg_train) + nrow(mpg_test) == nrow(mpg))# check if missed any row
mpg_trainnum=mpg_train[,numv]
mpg_lm=lm(mpg_trainnum$mpg ~ ., data =mpg_trainnum) #build linear model based on training data
summary(mpg_lm)
summary(model1)
numv
numVars
mpg = read.table('https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data', header = FALSE)
names(mpg) = c('mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'modelYear', 'origin', 'carName')
catv=names(mpg[,sapply(mpg,is.factor)])
catv
# [1] "horsepower" "carName"
numv=names(mpg[,sapply(mpg,is.numeric)])
numv
# [1]  "mpg"   "cylinders"    "displacement" "weight"    "acceleration" "modelYear"    "origin"
binv=names(mpg[,sapply(mpg,is.logical)])
binv
#character(0)
stopifnot(ncol(catv) + ncol(numv) +ncol(binv) == ncol(mpg)) # check if it includes all types data
# however, "horsepower" is numeric
catv=catv[2]
mpg[,'horsepower'] = as.numeric(mpg[,'horsepower'])
numv = c(numv, 'horsepower')
catv
catVars
numv
numVars
colnames(mpg)[colSums(is.na(mpg))>0]
colnames(df)[colSums(is.na(df))>0]
summary(mpg$horsepower)
View(mpg)
mpg = read.table('https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data', header = FALSE)
names(mpg) = c('mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'modelYear', 'origin', 'carName')
# 2. Identify all of the categorical variables, all of the numeric variables
# and all of the binary variables.
catv=names(mpg[,sapply(mpg,is.factor)])
catv
# [1] "horsepower" "carName"
numv=names(mpg[,sapply(mpg,is.numeric)])
numv
# [1]  "mpg"   "cylinders"    "displacement" "weight"    "acceleration" "modelYear"    "origin"
binv=names(mpg[,sapply(mpg,is.logical)])
binv
#character(0)
stopifnot(ncol(catv) + ncol(numv) +ncol(binv) == ncol(mpg)) # check if it includes all types data
colnames(mpg)[colSums(is.na(mpg))>0]
View(mpg)
colnames(mpg)[colSums(mpg=="?")>0]
# colunm "horsepower" has missing date
# however, "horsepower" is numeric
catv=catv[2]
mpg[,'horsepower'] = as.numeric(mpg[,'horsepower'])
numv = c(numv, 'horsepower')
catv
numv
numVars
library('caret')
set.seed(2575) # to make result reproducible
itrain = createDataPartition(y = mpg[,1], p = .8,list = FALSE)
mpg_train=mpg[itrain,]
mpg_test=mpg[-itrain,]
stopifnot(nrow(mpg_train) + nrow(mpg_test) == nrow(mpg))# check if missed any row
mpg_trainnum=mpg_train[,numv]
mpg_lm=lm(mpg_trainnum$mpg ~ ., data =mpg_trainnum) #build linear model based on training data
summary(mpg_lm)
mpg_testnum=mpg_test[,numv] #numerical varaible in test data set
y=mpg_testnum[,1] #actual target value in test data set
yhat=predict(mpg_lm,newdata = mpg_testnum)[,1] #predicted target value in test data set
rsq=sum((yhat - mean(y))**2)/sum((y - mean(y))**2) #calculate the e square
rsq
str(mpg_lm)
test[,'yHat'] <- predict(model1,  test)
test$yHat
yhat=predict(mpg_lm,newdata = mpg_testnum)
yhat
rsq=sum((yhat - mean(y))**2)/sum((y - mean(y))**2) #calculate the e square
rsq
summary(mpg_lm)$coef
sigv = names(which(pVals <=.05))
#from above model summary, we can know non-significant variable(alpha = .05)\
pVals = summary(mpg_lm)$coef[,4]
sigv = names(which(pVals <=.05))
sigv = sigv[2:length(sigv)] # Remove intercept
sigv
mpg_lm2=lm(mpg_trainnum$mpg ~ mpg_trainnum$weight+mpg_trainnum$acceleration+mpg_trainnum$modelYear+mpg_trainnum$origin, data = mpg_trainnum)
yhat2=predict(mpg_lm2,newdata = mpg_testnum)
mpg_lm2
summary(mpg_lm2)
mpg_lm2=lm(mpg_trainnum$mpg ~ "weight"+"acceleration"+"modelYear"+"origin", data = mpg_trainnum)
mpg_lm2=lm(mpg_trainnum$mpg ~ weight+acceleration+modelYear+origin, data = mpg_trainnum)
yhat2=predict(mpg_lm2,newdata = mpg_testnum)
rsq2=sum((yhat2 - mean(y))**2)/sum((y - mean(y))**2)
rsq2
mpg_lm3=lm(mpg_train$mpg ~., data = mpg_train)
yhat3=predict(mpg_lm3, newdata = mpg_test)
View(mpg)
mpg[,'horsepower'] = as.numeric(as.character(mpg[,'horsepower']))
mpg = read.table('https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data', header = FALSE)
names(mpg) = c('mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'modelYear', 'origin', 'carName')
catv=names(mpg[,sapply(mpg,is.factor)])
catv
# [1] "horsepower" "carName"
numv=names(mpg[,sapply(mpg,is.numeric)])
numv
# [1]  "mpg"   "cylinders"    "displacement" "weight"    "acceleration" "modelYear"    "origin"
binv=names(mpg[,sapply(mpg,is.logical)])
binv
#character(0)
stopifnot(ncol(catv) + ncol(numv) +ncol(binv) == ncol(mpg)) # check if it includes all types data
levels(mpg$carName)
# Too many levels in these two variables. Making contingency table is not a good idea.
mpg_ct=table(mpg$carName)
mpg_ct
colnames(mpg)[colSums(mpg=="?")>0]
library('caret')
set.seed(2575) # to make result reproducible
itrain = createDataPartition(y = mpg[,1], p = .8,list = FALSE)
mpg_train=mpg[itrain,]
mpg_test=mpg[-itrain,]
stopifnot(nrow(mpg_train) + nrow(mpg_test) == nrow(mpg))# check if missed any row
mpg_trainnum=mpg_train[,numv]
mpg_lm=lm(mpg_trainnum$mpg ~ ., data =mpg_trainnum) #build linear model based on training data
summary(mpg_lm)
mpg_testnum=mpg_test[,numv] #numerical varaible in test data set
y=mpg_testnum[,1] #actual target value in test data set
yhat=predict(mpg_lm,newdata = mpg_testnum)#predicted target value in test data set
rsq=sum((yhat - mean(y))**2)/sum((y - mean(y))**2) #calculate the e square
rsq
#from above model summary, we can know non-significant variable(alpha = .05)\
pVals = summary(mpg_lm)$coef[,4]
sigv = names(which(pVals <=.05))
sigv = sigv[2:length(sigv)] # Remove intercept
sigv
mpg_lm2=lm(mpg_trainnum$mpg ~ weight+acceleration+modelYear+origin, data = mpg_trainnum)
yhat2=predict(mpg_lm2,newdata = mpg_testnum)
rsq2=sum((yhat2 - mean(y))**2)/sum((y - mean(y))**2)
rsq2
mpg_lm3=lm(mpg_train$mpg ~., data = mpg_train)
yhat3=predict(mpg_lm3, newdata = mpg_test)
#To fix it.
# make "horsepower" is numeric
catv=catv[2]
catv
View(mpg)
mpg[,'horsepower'] = as.numeric(as.character(mpg$horsepower))
numv = c(numv, 'horsepower')
mpg_train2=mpg[itrain,-9]
mpg_test2=mpg[-itrain,-9]
mpg_lm4=lm(mpg_train2$mpg ~., data=mpg_train2)
y4=mpg_test2[,1]
yhat4=predict(mpg_lm4, newdata = mpg_test2)
rsq4=sum((yhat4 - mean(y4))**2)/sum((y4 - mean(y4))**2)
rsq4
mpg$mpg[is.na(mpg$mpg)] = mean(mpg$mpg,na.rm=T) #use mean value to replace missing value in V4
numv
mpg_lm4=lm(mpg_train2$mpg ~., data=mpg_train2)
y4=mpg_test2[,1]
yhat4=predict(mpg_lm4, newdata = mpg_test2)
rsq4=sum((yhat4 - mean(y4))**2)/sum((y4 - mean(y4))**2)
rsq4
yhat4
y4
length(y4)
length(yhat4)
class(yhat4)
class(y4)
sum(yhat4-y4)
is.na(yhat4)
sum(is.na(yhat4))
#fit linear model
mpg_lm4=lm(mpg_train$mpg ~., data=mpg_train)
y4=mpg_test[,1]
yhat4=predict(mpg_lm4, newdata = mpg_test)
class(mpg$horsepower)
summary(mpg$horsepower)
mpg$mpg[is.na(mpg$mpg)] = mean(mpg$mpg,na.rm=T) #use mean value to replace missing value in V4
summary(mpg$horsepower)
mpg$horsepower[is.na(mpg$horsepower)] = mean(mpg$horsepower,na.rm=T) #use mean value to replace missing value in V4
summary(mpg$horsepower)
#fit linear model
mpg_lm4=lm(mpg_train$mpg ~., data=mpg_train)
y4=mpg_test[,1]
yhat4=predict(mpg_lm4, newdata = mpg_test)
#fit linear model
#get new training and testing data
mpg_train2=mpg[itrain,]
mpg_test2=mpg[-itrain,]
mpg_lm4=lm(mpg_train$mpg ~., data=mpg_train2)
y4=mpg_test2[,1]
yhat4=predict(mpg_lm4, newdata = mpg_test2)
mpg_train2=mpg[itrain,-9]
mpg_test2=mpg[-itrain,-9]
mpg_lm4=lm(mpg_train$mpg ~., data=mpg_train2)
y4=mpg_test2[,1]
yhat4=predict(mpg_lm4, newdata = mpg_test2)
rsq4=sum((yhat4 - mean(y4))**2)/sum((y4 - mean(y4))**2)
rsq4
re = lm(mpg$mpg~mpg$modelYear, data=mpg)
re = lm(mpg~modelYear, data=mpg)
plot(mpg$modelYear,mpg$mpg)
abline(re) #add straight regression line
summary(re)
mpg_lmb2=lm(mpg_train2$mpg ~ ., data=mpg_train2)
y5=mpg_test2[,1]
yhat5=predict(mpg_lmb2, newdata = mpg_test2,interval = 'prediction')[,1]
rsq5=sum((yhat5 - mean(y5))**2)/sum((y5 - mean(y5))**2)
rsq5
mpg_lmb=lm(mpg_train2$mpg ~ ., data=mpg_train2)
y5=mpg_test2[,1]
yhat5=predict(mpg_lmb, newdata = mpg_test2,interval = 'prediction')[,1]
r5=sum((yhat5 - mean(y5))**2)/sum((y5 - mean(y5))**2)
r5
summary(mpg_lmb)
# selecting the varaibels showing strong dependencies.
mpg_lmb2=lm(mpg_trainnum$mpg ~ weight+displacement+modelYear+origin, data = mpg_train2)
yhat6=predict(mpg_lmb2,newdata = mpg_train2)
r6=sum((yhat6 - mean(y))**2)/sum((y - mean(y))**2)
r6
r6=sum((yhat6 - mean(y5))**2)/sum((y5 - mean(y5))**2)
r6
mpg_lmb=lm(mpg_train2$mpg ~ ., data=mpg_train2)
y5=mpg_test2[,1]
yhat5=predict(mpg_lmb, newdata = mpg_test2,interval = 'prediction')[,1]
r5=sum((yhat5 - mean(y5))**2)/sum((y5 - mean(y5))**2)
r5
mpg_lmb2=lm(mpg_trainnum$mpg ~ weight+displacement+modelYear+origin, data = mpg_train2)
yhat6=predict(mpg_lmb2,newdata = mpg_train2)
r6=sum((yhat6 - mean(y5))**2)/sum((y5 - mean(y5))**2)
r6
yhat6=predict(mpg_lmb2,newdata = mpg_test2)
r6=sum((yhat6 - mean(y5))**2)/sum((y5 - mean(y5))**2)
r6
#Read In Data
setwd('C:/Users/Documents/GitHub/18S571project/completedata')
#Read In Data
setwd('C:/Users/Documents/GitHub/18S571project/completedata')
setwd("~/Documents")
setwd("~/Documents/GitHub/18S571project/completedata")
#Read In Data
setwd('C:/Users/Documents/GitHub/18S571project/completedata')
water.data.raw<-read.csv('completeMwrd_BOD5.csv',header = TRUE,sep = ',',stringsAsFactors=FALSE)
rownames(water.data.raw)<-water.data.raw$X
water.data.raw<-water.data.raw[,-1]
head(water.data)
water.data<-water.data.raw[,-1]
head(water.data)
#Separate Date in 3 columns Year, Month, Day
water.data<-separate(water.data.raw,DATE,c('Year','Month','Day'),sep='-',remove=TRUE)
head(water.data)
water.data<-water.data[,-c(1,3)]
#Convert to Numeric for creating season variable
water.data$Month<-as.numeric(water.data$Month)
library('rvest')
library('tidyr')
library('caret')
library('Matrix')
library('foreach')
library('glmnet')
library('pls')
#Create model form function
createModelFormula <- function(TARGET, XVARS, includeIntercept = TRUE){
if(includeIntercept){
modelForm <- as.formula(paste(TARGET, "~", paste(XVARS, collapse = '+ ')))
} else {
modelForm <- as.formula(paste(TARGET, "~", paste( XVARS, collapse = '+ '), -1))
}
return(modelForm)
}
require(miscTools)
adjR2_test<-function (pred,y.test){
r2=miscTools::rSquared(y.test,resid = y.test-pred)
n<-nrow(x.test)
q<-ncol(x.test)
adjr2<-1-((1-r2^2)*(n-1)/(n-q))
}
get_Out<-function(data=water.data,row){
outlier<- water.data[c(row-1,row,row+1),]
}
#Global Variables
TARGET<-'BOD5'
XVARS<- c("TKN", 'NH3.N','P.TOT', 'SS', 'FLOW', 'Rainfall', 'Location','Season','Pop.density')
modelForm<-createModelFormula(TARGET,XVARS)
water.data.raw<-read.csv('completeMwrd_BOD5.csv',header = TRUE,sep = ',',stringsAsFactors=FALSE)
rownames(water.data.raw)<-water.data.raw$X
water.data<-water.data.raw[,-1]
head(water.data)
#Separate Date in 3 columns Year, Month, Day
water.data<-separate(water.data,DATE,c('Year','Month','Day'),sep='-',remove=TRUE)
head(water.data)
water.data<-water.data[,-c(1,3)]
#Convert to Numeric for creating season variable
water.data$Month<-as.numeric(water.data$Month)
#Convert Location to Factor
water.data$Location<-as.factor(water.data$Location)
#Create Season Variable
Season<-rep(0,nrow(water.data))
Season[water.data$Month>=1&water.data$Month<=3]<-1 #Winter
Season[water.data$Month>=4&water.data$Month<=6]<-2 #Spring
Season[water.data$Month>=7&water.data$Month<=9]<-3 #Summer
Season[water.data$Month>=10&water.data$Month<=12]<-4 #Fall
#Create Data frame, Make season and turn month back to factor for use in models
water.data<-data.frame(water.data,'Season'=as.factor(Season))
#water.data$Pop.density<-sapply(water.data$Pop.density,as.factor)
#Get rid of numeric month data
water.data<-water.data[,-1]
head(water.data)
#Check all the columns are the correct data type
sapply(water.data,class)
#Cor Matrix
cor(water.data[,c(1:7,9)])
corrplot::corrplot(water.data[,c(1:7,9)])
#Cor Matrix
c=cor(water.data[,c(1:7,9)])
corrplot::corrplot(c)
summary(water.data[,c(1:7,9)])
summary(water.data[,c(1:7,9)])
#Scale Data
water.data.scale<-scale(water.data[,c(1:7,9)],center=FALSE)
water.data.scale<-data.frame('Location'=water.data$Location,'Season'=water.data$Season,water.data.scale)
head(water.data.scale)
#Check data type
sapply(water.data.scale,class)
dim(water.data.scale)
#Create Training and Test Set
set.seed(571)
inTrain<-createDataPartition(y=water.data.scale[,TARGET],p=0.8,list=FALSE)
train<-water.data.scale[inTrain,]
test<-water.data.scale[-inTrain,]
y.test<-test[,TARGET]
x.test<-test[,XVARS]
# perform cross-validtion on training data set
controlParameter=trainControl(method = "cv",number = 10,savePredictions = TRUE)
# Models
set.seed(571)
lm_model<-train(modelForm,data=train,method='lm',trControl=controlParameter)
set.seed(571)
forward_model=train(modelForm,data=train,method='leapForward',trControl=controlParameter)
set.seed(571)
forward_model=train(modelForm,data=train,method='leapForward',trControl=controlParameter)
